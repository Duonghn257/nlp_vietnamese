{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.dataset import VietnameseTextDataset, prepare_vietnamese_dataset, load_texts_from_folder\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from glob import glob\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6269cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Sequence, Digits, Metaspace\n",
    "from tokenizers.normalizers import NFC, NFD, Sequence as NormalizerSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import os\n",
    "import glob\n",
    "from src.tokenizer import VietnameseTokenizer, VietnamesePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2706211",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'train_data_1/vietnam_poetry.txt'   # your input file\n",
    "output_filename = 'train_data_1/vietnam_poetry_cleaned.txt' # desired output file\n",
    "lower_filename = 'train_data_1/vietnam_poetry_lower.txt'\n",
    "segmented_filename = \"data1/segmented_output1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(segmented_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    segmented_output = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_output = segmented_output.split(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(segmented_output):\n",
    "    segmented_output[i] = segmented_output[i].lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94549441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for text in segmented_output[1:]:\n",
    "    if text.endswith(\"[LF]\"):\n",
    "        cnt+=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b18631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data1/segmented_output2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in segmented_output:\n",
    "        f.write(\"<s>\" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2dab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b33f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = VietnamesePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_tokenizer = VietnameseTokenizer()\n",
    "print(segmented_output[1])\n",
    "preprocessor.clean_text(segmented_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17850e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = load_texts_from_folder(\"data1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91188117",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(\n",
    "        sentences_from_file\n",
    "    )  # Use extend to add all sentences to one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d964ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7e8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97671efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "segment_tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\", model_max_length=512, truncation=True)\n",
    "model_segment = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "# segment_tokenizer.model_max_length = 512\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model_segment, tokenizer=segment_tokenizer, device=device)\n",
    "\n",
    "async def word_segment(text: str) -> str:\n",
    "    ner_results = nlp(text)\n",
    "    example_tok = \"\"\n",
    "    for e in ner_results:\n",
    "        if \"##\" in e[\"word\"]:\n",
    "            example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "        elif e[\"entity\"] ==\"I\":\n",
    "            example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "        else:\n",
    "            example_tok = example_tok + \" \" + e[\"word\"]\n",
    "    return example_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458ac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5442a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lower_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lower_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "async def is_lowercase_letter(char: str):\n",
    "    return unicodedata.category(char) == \"Ll\"\n",
    "\n",
    "async def capitalize(text: str):\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    first_char = text[0]\n",
    "    \n",
    "    if await is_lowercase_letter(first_char):\n",
    "        # Chuẩn hóa để đảm bảo nhất quán (NFC)\n",
    "        normalized = unicodedata.normalize('NFC', text)\n",
    "        return normalized.capitalize()\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "import re\n",
    "\n",
    "async def clean_text(text):\n",
    "    # Giữ lại: chữ cái, số, dấu câu cơ bản, khoảng trắng\n",
    "    # Loại bỏ: emoji, icon, ký tự đặc biệt\n",
    "    cleaned = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\n]', '', text)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(data):\n",
    "    if text.startswith(\"<s>thơ\"):\n",
    "        # text = text.strip()\n",
    "        continue\n",
    "    text = text.lstrip()\n",
    "    text = await capitalize(text)\n",
    "    text = await clean_text(text)\n",
    "    data[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in data:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_data = f.read()\n",
    "    data = f.readlines()\n",
    "\n",
    "print(len(clean_data), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.split(\"<s>\")\n",
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef53791",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Sequence, Value\n",
    "dataset = Dataset.from_list([{\"text\": text} for text in clean_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da77258",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e59a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    # Gọi pipeline với batch text\n",
    "    results = nlp(batch[\"text\"])  # pipeline tự xử lý batch\n",
    "\n",
    "    segmented_texts = []\n",
    "    for result in results:\n",
    "        example_tok = \"\"\n",
    "        for e in result:\n",
    "            if \"##\" in e[\"word\"]:\n",
    "                example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "            elif e[\"entity\"] ==\"I\":\n",
    "                example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "            else:\n",
    "                example_tok = example_tok + \" \" + e[\"word\"]\n",
    "        segmented_texts.append(example_tok)\n",
    "    \n",
    "    return {\"segmented_text\": segmented_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_segmented = dataset.map(process_batch, batched=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0825e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_segmented[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8064613",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_outputs = dataset_with_segmented[\"segmented_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_outputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d065b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_newline_before_capital(text):\n",
    "    # Dùng regex: tìm các từ bắt đầu bằng chữ cái in hoa (trừ vị trí đầu chuỗi)\n",
    "    # \\b = word boundary, [A-ZÀÁẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬĐEÉÈẺẼẸÊẾỀỂỄỆIÍÌỈĨỊOÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢUÚÙỦŨỤƯỨỪỬỮỰYÝỲỶỸỴ]\n",
    "    # Nhưng đơn giản: [A-Z] + các chữ cái tiếng Việt in hoa\n",
    "\n",
    "    # Danh sách chữ cái tiếng Việt in hoa\n",
    "    uppercase_vietnamese = 'AÁÀẠẢÃĂẮẰẶẲẴÂẤẦẬẨẪBCDĐEÉÈẸẺẼÊẾỀỆỂỄFGHIÍÌỊỈĨKLMNOÓÒỌỎÕÔỐỒỘỔỖƠỚỜỢỞỠPQRSTUÚÙỤỦŨƯỨỪỰỬỮVWXYÝỲỴỶỸ'\n",
    "    pattern = f'(?<!^)(\\\\b[{uppercase_vietnamese}][a-zàáảãạăắằẳẵặâấầẩẫậđèéẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợùúủũụưứừửữựýỳỷỹỵ_]+)'\n",
    "\n",
    "    # Thay thế: thêm \\n trước từ in hoa (trừ từ đầu)\n",
    "    result = re.sub(pattern, r'\\n\\1', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2942f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_output = []\n",
    "for text in segmented_outputs:\n",
    "    new_text = add_newline_before_capital(text)\n",
    "    segmented_output.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c673d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571715a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17605e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"train_data_1/segmented_output.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in segmented_output:\n",
    "        f.write(\"<s>\" + line + \"\\n\")\n",
    "\n",
    "print(f\"✅ Đã xử lý và lưu {len(segmented_output)} dòng vào '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def preprocessing():\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for text in data:\n",
    "            if text.startswith(\"<s>thơ\"):\n",
    "                outfile.write(text)\n",
    "                continue\n",
    "            text = text.strip()\n",
    "            # text = unicodedata.normalize('NFC', text)\n",
    "            text = await capitalize(text)\n",
    "            text = await clean_text(text)\n",
    "            # text = await word_segment(text)\n",
    "            outfile.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1575eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa02481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea28481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eefdf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c6934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae76bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e018bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece, BPE\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence\n",
    "from tokenizers.normalizers import Lowercase, NFD, NFC, Strip, StripAccents, Sequence as NormalizerSequence\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Khởi tạo tokenizer với mô hình WordPiece\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 2. Chuẩn hóa văn bản (rất quan trọng với tiếng Việt)\n",
    "tokenizer.normalizer = NormalizerSequence([\n",
    "    Strip(),  # Remove leading/trailing whitespace\n",
    "    NFC(),  # Normalize Vietnamese diacritics properly\n",
    "])\n",
    "\n",
    "# ⚠️ Nếu bạn muốn giữ dấu thanh (phù hợp với thơ), hãy BỎ Lowercase và StripAccents\n",
    "# → Chỉ dùng NFD() để chuẩn hóa Unicode\n",
    "\n",
    "# 3. Pre-tokenizer: tách theo khoảng trắng và dấu câu\n",
    "tokenizer.pre_tokenizer = Sequence([Whitespace()])\n",
    "\n",
    "# 4. Trainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=25000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[BOS]\", \"[EOS]\", \"[LF]\"],\n",
    "    show_progress=True,\n",
    "    initial_alphabet = [\n",
    "        # Chữ thường có dấu\n",
    "        \"a\", \"á\", \"à\", \"ả\", \"ã\", \"ạ\",\n",
    "        \"ă\", \"ắ\", \"ằ\", \"ẳ\", \"ẵ\", \"ặ\",\n",
    "        \"â\", \"ấ\", \"ầ\", \"ẩ\", \"ẫ\", \"ậ\",\n",
    "        \"b\", \"c\", \"d\", \"đ\",\n",
    "        \"e\", \"é\", \"è\", \"ẻ\", \"ẽ\", \"ẹ\",\n",
    "        \"ê\", \"ế\", \"ề\", \"ể\", \"ễ\", \"ệ\",\n",
    "        \"f\", \"g\", \"h\", \"i\", \"í\", \"ì\", \"ỉ\", \"ĩ\", \"ị\",\n",
    "        \"k\", \"l\", \"m\", \"n\",\n",
    "        \"o\", \"ó\", \"ò\", \"ỏ\", \"õ\", \"ọ\",\n",
    "        \"ô\", \"ố\", \"ồ\", \"ổ\", \"ỗ\", \"ộ\",\n",
    "        \"ơ\", \"ớ\", \"ờ\", \"ở\", \"ỡ\", \"ợ\",\n",
    "        \"p\", \"q\", \"r\", \"s\", \"t\",\n",
    "        \"u\", \"ú\", \"ù\", \"ủ\", \"ũ\", \"ụ\",\n",
    "        \"ư\", \"ứ\", \"ừ\", \"ử\", \"ữ\", \"ự\",\n",
    "        \"v\", \"x\", \"y\", \"ý\", \"ỳ\", \"ỷ\", \"ỹ\", \"ỵ\",\n",
    "\n",
    "        # Chữ HOA có dấu\n",
    "        \"A\", \"Á\", \"À\", \"Ả\", \"Ã\", \"Ạ\",\n",
    "        \"Ă\", \"Ắ\", \"Ằ\", \"Ẳ\", \"Ẵ\", \"Ặ\",\n",
    "        \"Â\", \"Ấ\", \"Ầ\", \"Ẩ\", \"Ẫ\", \"Ậ\",\n",
    "        \"B\", \"C\", \"D\", \"Đ\",\n",
    "        \"E\", \"É\", \"È\", \"Ẻ\", \"Ẽ\", \"Ẹ\",\n",
    "        \"Ê\", \"Ế\", \"Ề\", \"Ể\", \"Ễ\", \"Ệ\",\n",
    "        \"F\", \"G\", \"H\", \"I\", \"Í\", \"Ì\", \"Ỉ\", \"Ĩ\", \"Ị\",\n",
    "        \"K\", \"L\", \"M\", \"N\",\n",
    "        \"O\", \"Ó\", \"Ò\", \"Ỏ\", \"Õ\", \"Ọ\",\n",
    "        \"Ô\", \"Ố\", \"Ồ\", \"Ổ\", \"Ỗ\", \"Ộ\",\n",
    "        \"Ơ\", \"Ớ\", \"Ờ\", \"Ở\", \"Ỡ\", \"Ợ\",\n",
    "        \"P\", \"Q\", \"R\", \"S\", \"T\",\n",
    "        \"U\", \"Ú\", \"Ù\", \"Ủ\", \"Ũ\", \"Ụ\",\n",
    "        \"Ư\", \"Ứ\", \"Ừ\", \"Ử\", \"Ữ\", \"Ự\",\n",
    "        \"V\", \"X\", \"Y\", \"Ý\", \"Ỳ\", \"Ỷ\", \"Ỹ\", \"Ỵ\"\n",
    "    ],\n",
    "    continuing_subword_prefix=\"##\"  # Dùng cho subword tiếp theo (ví dụ: \"sinh\" → \"sin\", \"##h\")\n",
    ")\n",
    "\n",
    "# 5. Dữ liệu huấn luyện\n",
    "train_files = glob.glob(\"./data1/*.txt\")\n",
    "if not train_files:\n",
    "    raise FileNotFoundError(\"Không tìm thấy file dữ liệu trong thư mục ./train_data\")\n",
    "\n",
    "# 6. Huấn luyện\n",
    "tokenizer.train(files=train_files, trainer=trainer)\n",
    "\n",
    "# 7. Lưu tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "print(\"✅ WordPiece tokenizer đã được lưu vào 'wordpiece_tokenizer.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(await word_segment(\"ai ơi xa bến quê hương nhớ về quê mẹ nắng sương đây nè\"), add_special_tokens=True)\n",
    "tokens.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a504203",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokens.ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951de40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = VietnamesePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data1\"\n",
    "raw_texts = load_texts_from_folder(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(\n",
    "        sentences_from_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b08a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(\n",
    "        sentences_from_file\n",
    "    )  # Use extend to add all sentences to one list\n",
    "\n",
    "# print(all_sentences)\n",
    "print(f\"   - Number of sentences: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbf6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_tokenizer = VietnameseTokenizer()\n",
    "tokenizer = vn_tokenizer.load(\"wordpiece_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b18e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VietnameseTextDataset(all_sentences, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset.__getitem__(1)\n",
    "ids = sample['input_ids']\n",
    "print(tokenizer.decode(ids.tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer with Vietnamese-specific settings\n",
    "trainer = vn_tokenizer.build_tokenizer(\n",
    "    vocab_size=20000,\n",
    "    min_frequency=2\n",
    ")\n",
    "\n",
    "# Get training files\n",
    "train_files = glob.glob(os.path.join(\"./data1\", \"*.txt\"))\n",
    "\n",
    "if not train_files:\n",
    "    print(\"No training files found in ./train_data/\")\n",
    "    print(\"Please add Vietnamese text files (.txt) to ./train_data/ directory\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(train_files)} training files\")\n",
    "\n",
    "# Train the tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "vn_tokenizer.train(train_files, trainer)\n",
    "\n",
    "# Setup post-processor\n",
    "vn_tokenizer.setup_post_processor()\n",
    "\n",
    "# Save tokenizer\n",
    "vn_tokenizer.save(\"vietnamese_enhanced_tokenizer.json\")\n",
    "print(\"Tokenizer saved as 'vietnamese_enhanced_tokenizer.json'\")\n",
    "\n",
    "# Test the tokenizer\n",
    "vn_tokenizer.test_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06863b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize(tokenizer, text: str):\n",
    "    text = word_segment(text)\n",
    "    tokens = tokenizer.encode(text).ids\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = test_tokenize(vn_tokenizer, \"ai ơi xa bến quê hương nhớ về quê mẹ nắng sương đây nè\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_tokenizer.decode(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
