{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4199595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.dataset import VietnameseTextDataset, prepare_vietnamese_dataset, load_texts_from_folder\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from glob import glob\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5759c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Sequence, Digits, Metaspace\n",
    "from tokenizers.normalizers import NFC, NFD, Sequence as NormalizerSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import os\n",
    "import glob\n",
    "from src.tokenizer import VietnameseTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274d745",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "**Segment Word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d18160",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"thơ lục bát: \n",
    "ai ơi xa bến quê hương\n",
    "nhớ về quê mẹ nắng sương đây nè\n",
    "nhớ sao những buổi trưa hè\n",
    "võng đưa cót két gió hè hiu hiu\n",
    "lời ru của mẹ dấu yêu\n",
    "ngọt ngào êm dịu mẹ yêu con nhiều\n",
    "lời mẹ năm tháng sớm chiều\n",
    "con nghe nhớ mãi những điều ru ta\n",
    "lòng mẹ ôi thật bao la\n",
    "thái bình rộng lớn thương là mênh mông\n",
    "một đời lưng mẹ đã còng\n",
    "vai mang tay xách lo chồng chăm con\n",
    "mong sao con lớn nên người\n",
    "thân mẹ có cực vẫn cười thương con\n",
    "vì đời mẹ sống cho con\n",
    "gom bao mệt nhọc mãi còn vòng tay\n",
    "mong chờ cho đến một ngày\n",
    "công thành danh toại là ngày mẹ mong\n",
    "dù nay mẹ đã xa rồi\n",
    "con đây nhớ mãi mẹ ôi vạn lần\n",
    "nguyện rằng ghi nhớ công ân\n",
    "sinh thành dưỡng dục mẫu thân đời đời\"\"\"\n",
    "\n",
    "def word_segment(text: str) -> str:\n",
    "    ner_results = nlp(text)\n",
    "    example_tok = \"\"\n",
    "    for e in ner_results:\n",
    "        if \"##\" in e[\"word\"]:\n",
    "            example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "        elif e[\"entity\"] ==\"I\":\n",
    "            example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "        else:\n",
    "            example_tok = example_tok + \" \" + e[\"word\"]\n",
    "    return example_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa93e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'train_data_1/vietnam_poetry.txt'   # your input file\n",
    "output_filename = 'train_data_1/vietnam_poetry_3.txt' # desired output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1794ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "new_data = data.split(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53bb4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27b1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_poetry_data(data, output_file, max_words=512):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for stanza in data:  # Mỗi stanza là một phần tử trong danh sách\n",
    "            if not isinstance(stanza, str) or not stanza.strip():\n",
    "                continue  # Bỏ qua nếu không phải chuỗi hoặc rỗng\n",
    "\n",
    "            words = stanza.strip().split()\n",
    "            \n",
    "            # Chia thành các chunk ≤ 512 từ\n",
    "            for i in range(0, len(words), max_words):\n",
    "                chunk = ' '.join(words[i:i + max_words])\n",
    "                f.write(f\"<s> {chunk}\\n\")\n",
    "    \n",
    "    print(f\"Đã lưu {len(data)} khổ thơ (đã chia nhỏ) vào {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b08f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 171189 khổ thơ (đã chia nhỏ) vào train_data_1/vietnam_poetry_3.txt\n"
     ]
    }
   ],
   "source": [
    "save_poetry_data(new_data, output_filename, max_words=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b3672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in new_data[1:]:\n",
    "        print(line)\n",
    "        text = word_segment(line)\n",
    "        outfile.write(\"<s>\" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ed7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_tokenizer = VietnameseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer with Vietnamese-specific settings\n",
    "trainer = vn_tokenizer.build_tokenizer(\n",
    "    vocab_size=25000,\n",
    "    min_frequency=2\n",
    ")\n",
    "\n",
    "# Get training files\n",
    "train_files = glob.glob(os.path.join(\"./train_data\", \"*.txt\"))\n",
    "\n",
    "if not train_files:\n",
    "    print(\"No training files found in ./train_data/\")\n",
    "    print(\"Please add Vietnamese text files (.txt) to ./train_data/ directory\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(train_files)} training files\")\n",
    "\n",
    "# Train the tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "vn_tokenizer.train(train_files, trainer)\n",
    "\n",
    "# Setup post-processor\n",
    "vn_tokenizer.setup_post_processor()\n",
    "\n",
    "# Save tokenizer\n",
    "vn_tokenizer.save(\"vietnamese_enhanced_tokenizer.json\")\n",
    "print(\"Tokenizer saved as 'vietnamese_enhanced_tokenizer.json'\")\n",
    "\n",
    "# Test the tokenizer\n",
    "vn_tokenizer.test_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35645161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a02512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7600581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_tokens(string: str, model_name: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string for a specific model.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_data/vietnam_poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68779212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_tokens(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9496237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. KHỞI TẠO TOKENIZER CHUẨN\n",
    "tokenizer = Tokenizer(BPE(\n",
    "    unk_token=\"[UNK]\",\n",
    "    fuse_unk=True  # Tự động gộp các UNK hiếm\n",
    "))\n",
    "\n",
    "# 2. NORMALIZER QUAN TRỌNG NHẤT (FIX DẤU THANH)\n",
    "tokenizer.normalizer = NormalizerSequence([\n",
    "    NFC(),  # ⚡️ CHÌA KHÓA VÀNG: Giữ nguyên dấu thanh (Á = 1 ký tự, KHÔNG tách A + dấu)\n",
    "    NFD(),  # Tối ưu cho 1 số trường hợp hiếm\n",
    "])\n",
    "\n",
    "# 3. PRE-TOKENIZER CHUYÊN BIỆT CHO TIẾNG VIỆT\n",
    "tokenizer.pre_tokenizer = Sequence([\n",
    "    Digits(individual_digits=True),  # Tách số riêng (123 → [\"1\",\"2\",\"3\"])\n",
    "    Punctuation(),                   # Tách DẤU CÂU đúng cách (.,?!;:→ riêng)\n",
    "    Metaspace(replacement=\"▁\")  # Dùng▁ thay dấu cách (phù hợp BPE)\n",
    "])\n",
    "\n",
    "# 4. TRAINER VỚI THAM SỐ TỐI ƯU\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=22000,          # Tăng nhẹ từ 20K → 22K để đủ chỗ cho dấu câu\n",
    "    min_frequency=3,           # Chỉ lấy token xuất hiện ≥ 3 lần\n",
    "    special_tokens=[\n",
    "        \"[PAD]\", \"[UNK]\", \n",
    "        \"[EOS]\", \"[BOS]\",\n",
    "        \"[MASK]\"               # Cần cho 1 số kỹ thuật augment\n",
    "    ],\n",
    "    # initial_alphabet=[\"▁\"],    # Ký tự bắt đầu từ (Metaspace)\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# 5. TRAINING (DÙNG TẤT CẢ FILE .txt)\n",
    "files = glob.glob(os.path.join(\"./train_data\", \"*.txt\"))\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 6. POST-PROCESSOR (QUAN TRỌNG CHO GENERATION)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. LƯU VÀ KIỂM TRA\n",
    "tokenizer.save(\"vietnamese_pro_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed253dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"vietnamese_pro_tokenizer.json\")\n",
    "tokenizer = vn_tokenizer.load(\"vietnamese_enhanced_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1aa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"bướm xòe đôi cánh lả lơi\n",
    "tìm hoa hút mật tuyệt vời thơm ngon\n",
    "nắng chiều mơn trớn mạ non\n",
    "hoàng hôn tắt lịm trăng tròn tắm sông\n",
    "xuân về hạ tới thu đông\n",
    "xong mưa trời tặng cầu vồng trên cao\n",
    "mía dừa múi mít ngọt ngào\n",
    "chanh cam bí mướp mận đào xoài nho\"\"\"\n",
    "output = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93fca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokenizer.decode(output.ids, skip_special_tokens=True)\n",
    "print(\"Decoded Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer_hf = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"vietnamese_bytelevel_tokenizer.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09237082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    encoding = tokenizer.encode(text)\n",
    "    return len(encoding.tokens) # trả về số lượng và danh sách token (tuỳ chọn)\n",
    "\n",
    "# Ví dụ\n",
    "text = \"Xin chào, mình là sinh viên trường đại học Khoa học Tự nhiên.\"\n",
    "\n",
    "num_tokens = count_tokens(data)\n",
    "print(f\"Số lượng token: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691b271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a872a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(\"./data\", \"*.txt\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for data_file in files:\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "        sentences.append(raw_text)\n",
    "# sentences[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import VietnamesePreprocessor\n",
    "import random\n",
    "preprocessor = VietnamesePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = load_texts_from_folder(\"train_data_1\")\n",
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(sentences_from_file) # Use extend to add all sentences to one list\n",
    "\n",
    "train_split = 0.8\n",
    "random.shuffle(all_sentences)\n",
    "split_idx = int(len(all_sentences) * train_split)\n",
    "train_sentences = all_sentences[:split_idx]\n",
    "val_sentences = all_sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb69998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7aa3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = VietnameseTextDataset(texts=train_sentences, tokenizer=tokenizer, max_length=256, stride=128)\n",
    "train_data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce44284",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VietnameseTextDataset(texts=sentences, tokenizer=tokenizer, max_length=256, stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afe687",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_texts_from_folder(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = prepare_vietnamese_dataset(data_folder=\"train_data_1\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 2:  # Only show first 2 batches\n",
    "        break\n",
    "        \n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Target IDs shape: {batch['target_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show first sequence in batch\n",
    "    input_seq = batch['input_ids'][2]\n",
    "    target_seq = batch['target_ids'][2]\n",
    "    \n",
    "    print(f\"  Sample input:  {input_seq[:50].tolist()}...\")\n",
    "    print(f\"  Sample target: {target_seq[:50].tolist()}...\")\n",
    "    \n",
    "    # Decode sample\n",
    "    decoded_input = tokenizer.decode(input_seq.tolist())\n",
    "    decoded_target = tokenizer.decode(target_seq.tolist())\n",
    "    \n",
    "    print(f\"  Decoded input:  '{decoded_input[:50]}...'\")\n",
    "    print(f\"  Decoded target: '{decoded_target[:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data configuration\n",
    "    'data_folder': 'data1',\n",
    "    'tokenizer_file': 'vietnamese_bpe_tokenizer.json',\n",
    "    'vocab_size': 25000,\n",
    "    'max_seq_len': 512,\n",
    "    'train_split': 0.8,\n",
    "    \n",
    "    # Model configuration\n",
    "    'd_model': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'd_ff': 3072,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Training configuration\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 3e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 20,\n",
    "    'warmup_steps': 5000,\n",
    "    'device': 'auto',  # 'cuda', 'cpu', or 'auto'\n",
    "    \n",
    "    # Generation configuration\n",
    "    'temperature': 0.8,\n",
    "    'top_k': 10,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 100,\n",
    "    \n",
    "    # Save configuration\n",
    "    'model_save_path': 'vietnamese_transformer_best.pt',\n",
    "    'config_save_path': 'training_config.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import VietnameseTransformer\n",
    "\n",
    "model = VietnameseTransformer(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_len=config['max_seq_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_token_id=tokenizer.token_to_id(\"[PAD]\")\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "print(f\"✅ Model created successfully!\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import VietnameseTrainer\n",
    "from src.helpers import test_generation\n",
    "# Step 3: Initialize trainer\n",
    "print(f\"\\n{'='*20} STEP 3: TRAINING SETUP {'='*20}\")\n",
    "\n",
    "trainer = VietnameseTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "print(f\"✅ Trainer initialized!\")\n",
    "print(f\"   Device: {trainer.device}\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "\n",
    "# Test initial generation (before training)\n",
    "print(f\"\\n{'='*20} INITIAL GENERATION TEST {'='*20}\")\n",
    "print(\"Testing generation before training (should be random):\")\n",
    "test_generation(model, tokenizer, trainer.device, [\"thơ lục bát: ai ơi xa bến quê hương\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "print(f\"\\n{'='*20} STEP 4: TRAINING {'='*20}\")\n",
    "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "print(\"Press Ctrl+C to stop training early\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train(\n",
    "        num_epochs=config['num_epochs'],\n",
    "        save_path=config['model_save_path']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n⏹️  Training interrupted by user\")\n",
    "    print(\"Saving current model state...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'train_losses': trainer.train_losses,\n",
    "        'val_losses': trainer.val_losses,\n",
    "        # 'tokenizer': tokenizer\n",
    "    }, 'vietnamese_transformer_interrupted.pt')\n",
    "    print(\"Model saved as 'vietnamese_transformer_interrupted.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26652626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Final generation test\n",
    "print(f\"\\n{'='*20} STEP 6: FINAL GENERATION TEST {'='*20}\")\n",
    "print(config['model_save_path'])\n",
    "# Load best model for testing\n",
    "if os.path.exists(config['model_save_path']):\n",
    "    # Load the entire checkpoint dictionary without the `weights_only` flag\n",
    "    checkpoint = torch.load(config['model_save_path'], map_location=trainer.device, weights_only=True)\n",
    "\n",
    "    # Load only the model's weights from the loaded dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"✅ Loaded best model for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer, device, test_cases=None):\n",
    "    \"\"\"Test text generation with various examples\"\"\"\n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            \"Truyện Kiều là\",\n",
    "        ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 TESTING TEXT GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, prompt in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        print(f\"Input: '{prompt}'\")\n",
    "        \n",
    "        # Encode input\n",
    "        input_ids = torch.tensor(\n",
    "            [tokenizer.encode(prompt, add_special_tokens=False).ids],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Generate with different settings\n",
    "        generation_configs = [\n",
    "            {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'max_new_tokens': 15, 'name': 'Balanced'},\n",
    "            {'temperature': 1.0, 'top_k': 20, 'top_p': 0.8, 'max_new_tokens': 15, 'name': 'Creative'},\n",
    "            {'temperature': 0.0, 'top_k': 5, 'top_p': 1.0, 'max_new_tokens': 15, 'name': 'Conservative'}\n",
    "        ]\n",
    "        \n",
    "        for config in generation_configs:\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    temperature=config['temperature'],\n",
    "                    top_k=config['top_k'],\n",
    "                    top_p=config['top_p'],\n",
    "                    max_new_tokens=config['max_new_tokens'],\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "            print(f\"  {config['name']}: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation(model, tokenizer, device=\"cuda\", test_cases = [\"Sóng được sáng tác bởi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
