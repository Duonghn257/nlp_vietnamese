{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4199595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.dataset import VietnameseTextDataset, prepare_vietnamese_dataset, load_texts_from_folder\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from glob import glob\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5759c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Sequence, Digits, Metaspace\n",
    "from tokenizers.normalizers import NFC, NFD, Sequence as NormalizerSequence\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import os\n",
    "import glob\n",
    "from src.tokenizer import VietnameseTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274d745",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "**Segment Word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d18160",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"th∆° l·ª•c b√°t: \n",
    "ai ∆°i xa b·∫øn qu√™ h∆∞∆°ng\n",
    "nh·ªõ v·ªÅ qu√™ m·∫π n·∫Øng s∆∞∆°ng ƒë√¢y n√®\n",
    "nh·ªõ sao nh·ªØng bu·ªïi tr∆∞a h√®\n",
    "v√µng ƒë∆∞a c√≥t k√©t gi√≥ h√® hiu hiu\n",
    "l·ªùi ru c·ªßa m·∫π d·∫•u y√™u\n",
    "ng·ªçt ng√†o √™m d·ªãu m·∫π y√™u con nhi·ªÅu\n",
    "l·ªùi m·∫π nƒÉm th√°ng s·ªõm chi·ªÅu\n",
    "con nghe nh·ªõ m√£i nh·ªØng ƒëi·ªÅu ru ta\n",
    "l√≤ng m·∫π √¥i th·∫≠t bao la\n",
    "th√°i b√¨nh r·ªông l·ªõn th∆∞∆°ng l√† m√™nh m√¥ng\n",
    "m·ªôt ƒë·ªùi l∆∞ng m·∫π ƒë√£ c√≤ng\n",
    "vai mang tay x√°ch lo ch·ªìng chƒÉm con\n",
    "mong sao con l·ªõn n√™n ng∆∞·ªùi\n",
    "th√¢n m·∫π c√≥ c·ª±c v·∫´n c∆∞·ªùi th∆∞∆°ng con\n",
    "v√¨ ƒë·ªùi m·∫π s·ªëng cho con\n",
    "gom bao m·ªát nh·ªçc m√£i c√≤n v√≤ng tay\n",
    "mong ch·ªù cho ƒë·∫øn m·ªôt ng√†y\n",
    "c√¥ng th√†nh danh to·∫°i l√† ng√†y m·∫π mong\n",
    "d√π nay m·∫π ƒë√£ xa r·ªìi\n",
    "con ƒë√¢y nh·ªõ m√£i m·∫π √¥i v·∫°n l·∫ßn\n",
    "nguy·ªán r·∫±ng ghi nh·ªõ c√¥ng √¢n\n",
    "sinh th√†nh d∆∞·ª°ng d·ª•c m·∫´u th√¢n ƒë·ªùi ƒë·ªùi\"\"\"\n",
    "\n",
    "def word_segment(text: str) -> str:\n",
    "    ner_results = nlp(text)\n",
    "    example_tok = \"\"\n",
    "    for e in ner_results:\n",
    "        if \"##\" in e[\"word\"]:\n",
    "            example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "        elif e[\"entity\"] ==\"I\":\n",
    "            example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "        else:\n",
    "            example_tok = example_tok + \" \" + e[\"word\"]\n",
    "    return example_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa93e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'train_data_1/vietnam_poetry.txt'   # your input file\n",
    "output_filename = 'train_data_1/vietnam_poetry_3.txt' # desired output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1794ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "new_data = data.split(\"<s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53bb4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27b1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_poetry_data(data, output_file, max_words=512):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for stanza in data:  # M·ªói stanza l√† m·ªôt ph·∫ßn t·ª≠ trong danh s√°ch\n",
    "            if not isinstance(stanza, str) or not stanza.strip():\n",
    "                continue  # B·ªè qua n·∫øu kh√¥ng ph·∫£i chu·ªói ho·∫∑c r·ªóng\n",
    "\n",
    "            words = stanza.strip().split()\n",
    "            \n",
    "            # Chia th√†nh c√°c chunk ‚â§ 512 t·ª´\n",
    "            for i in range(0, len(words), max_words):\n",
    "                chunk = ' '.join(words[i:i + max_words])\n",
    "                f.write(f\"<s> {chunk}\\n\")\n",
    "    \n",
    "    print(f\"ƒê√£ l∆∞u {len(data)} kh·ªï th∆° (ƒë√£ chia nh·ªè) v√†o {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b08f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ l∆∞u 171189 kh·ªï th∆° (ƒë√£ chia nh·ªè) v√†o train_data_1/vietnam_poetry_3.txt\n"
     ]
    }
   ],
   "source": [
    "save_poetry_data(new_data, output_filename, max_words=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b3672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in new_data[1:]:\n",
    "        print(line)\n",
    "        text = word_segment(line)\n",
    "        outfile.write(\"<s>\" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ed7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_tokenizer = VietnameseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer with Vietnamese-specific settings\n",
    "trainer = vn_tokenizer.build_tokenizer(\n",
    "    vocab_size=25000,\n",
    "    min_frequency=2\n",
    ")\n",
    "\n",
    "# Get training files\n",
    "train_files = glob.glob(os.path.join(\"./train_data\", \"*.txt\"))\n",
    "\n",
    "if not train_files:\n",
    "    print(\"No training files found in ./train_data/\")\n",
    "    print(\"Please add Vietnamese text files (.txt) to ./train_data/ directory\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(train_files)} training files\")\n",
    "\n",
    "# Train the tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "vn_tokenizer.train(train_files, trainer)\n",
    "\n",
    "# Setup post-processor\n",
    "vn_tokenizer.setup_post_processor()\n",
    "\n",
    "# Save tokenizer\n",
    "vn_tokenizer.save(\"vietnamese_enhanced_tokenizer.json\")\n",
    "print(\"Tokenizer saved as 'vietnamese_enhanced_tokenizer.json'\")\n",
    "\n",
    "# Test the tokenizer\n",
    "vn_tokenizer.test_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35645161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a02512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7600581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_tokens(string: str, model_name: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string for a specific model.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_data/vietnam_poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68779212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_tokens(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9496237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. KH·ªûI T·∫†O TOKENIZER CHU·∫®N\n",
    "tokenizer = Tokenizer(BPE(\n",
    "    unk_token=\"[UNK]\",\n",
    "    fuse_unk=True  # T·ª± ƒë·ªông g·ªôp c√°c UNK hi·∫øm\n",
    "))\n",
    "\n",
    "# 2. NORMALIZER QUAN TR·ªåNG NH·∫§T (FIX D·∫§U THANH)\n",
    "tokenizer.normalizer = NormalizerSequence([\n",
    "    NFC(),  # ‚ö°Ô∏è CH√åA KH√ìA V√ÄNG: Gi·ªØ nguy√™n d·∫•u thanh (√Å = 1 k√Ω t·ª±, KH√îNG t√°ch A + d·∫•u)\n",
    "    NFD(),  # T·ªëi ∆∞u cho 1 s·ªë tr∆∞·ªùng h·ª£p hi·∫øm\n",
    "])\n",
    "\n",
    "# 3. PRE-TOKENIZER CHUY√äN BI·ªÜT CHO TI·∫æNG VI·ªÜT\n",
    "tokenizer.pre_tokenizer = Sequence([\n",
    "    Digits(individual_digits=True),  # T√°ch s·ªë ri√™ng (123 ‚Üí [\"1\",\"2\",\"3\"])\n",
    "    Punctuation(),                   # T√°ch D·∫§U C√ÇU ƒë√∫ng c√°ch (.,?!;:‚Üí ri√™ng)\n",
    "    Metaspace(replacement=\"‚ñÅ\")  # D√πng‚ñÅ thay d·∫•u c√°ch (ph√π h·ª£p BPE)\n",
    "])\n",
    "\n",
    "# 4. TRAINER V·ªöI THAM S·ªê T·ªêI ∆ØU\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=22000,          # TƒÉng nh·∫π t·ª´ 20K ‚Üí 22K ƒë·ªÉ ƒë·ªß ch·ªó cho d·∫•u c√¢u\n",
    "    min_frequency=3,           # Ch·ªâ l·∫•y token xu·∫•t hi·ªán ‚â• 3 l·∫ßn\n",
    "    special_tokens=[\n",
    "        \"[PAD]\", \"[UNK]\", \n",
    "        \"[EOS]\", \"[BOS]\",\n",
    "        \"[MASK]\"               # C·∫ßn cho 1 s·ªë k·ªπ thu·∫≠t augment\n",
    "    ],\n",
    "    # initial_alphabet=[\"‚ñÅ\"],    # K√Ω t·ª± b·∫Øt ƒë·∫ßu t·ª´ (Metaspace)\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# 5. TRAINING (D√ôNG T·∫§T C·∫¢ FILE .txt)\n",
    "files = glob.glob(os.path.join(\"./train_data\", \"*.txt\"))\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 6. POST-PROCESSOR (QUAN TR·ªåNG CHO GENERATION)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. L∆ØU V√Ä KI·ªÇM TRA\n",
    "tokenizer.save(\"vietnamese_pro_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed253dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file(\"vietnamese_pro_tokenizer.json\")\n",
    "tokenizer = vn_tokenizer.load(\"vietnamese_enhanced_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1aa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"b∆∞·ªõm x√≤e ƒë√¥i c√°nh l·∫£ l∆°i\n",
    "t√¨m hoa h√∫t m·∫≠t tuy·ªát v·ªùi th∆°m ngon\n",
    "n·∫Øng chi·ªÅu m∆°n tr·ªõn m·∫° non\n",
    "ho√†ng h√¥n t·∫Øt l·ªãm trƒÉng tr√≤n t·∫Øm s√¥ng\n",
    "xu√¢n v·ªÅ h·∫° t·ªõi thu ƒë√¥ng\n",
    "xong m∆∞a tr·ªùi t·∫∑ng c·∫ßu v·ªìng tr√™n cao\n",
    "m√≠a d·ª´a m√∫i m√≠t ng·ªçt ng√†o\n",
    "chanh cam b√≠ m∆∞·ªõp m·∫≠n ƒë√†o xo√†i nho\"\"\"\n",
    "output = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93fca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokenizer.decode(output.ids, skip_special_tokens=True)\n",
    "print(\"Decoded Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer_hf = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"vietnamese_bytelevel_tokenizer.json\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09237082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    encoding = tokenizer.encode(text)\n",
    "    return len(encoding.tokens) # tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng v√† danh s√°ch token (tu·ª≥ ch·ªçn)\n",
    "\n",
    "# V√≠ d·ª•\n",
    "text = \"Xin ch√†o, m√¨nh l√† sinh vi√™n tr∆∞·ªùng ƒë·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n.\"\n",
    "\n",
    "num_tokens = count_tokens(data)\n",
    "print(f\"S·ªë l∆∞·ª£ng token: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691b271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a872a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(\"./data\", \"*.txt\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for data_file in files:\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "        sentences.append(raw_text)\n",
    "# sentences[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import VietnamesePreprocessor\n",
    "import random\n",
    "preprocessor = VietnamesePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = load_texts_from_folder(\"train_data_1\")\n",
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(sentences_from_file) # Use extend to add all sentences to one list\n",
    "\n",
    "train_split = 0.8\n",
    "random.shuffle(all_sentences)\n",
    "split_idx = int(len(all_sentences) * train_split)\n",
    "train_sentences = all_sentences[:split_idx]\n",
    "val_sentences = all_sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb69998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7aa3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = VietnameseTextDataset(texts=train_sentences, tokenizer=tokenizer, max_length=256, stride=128)\n",
    "train_data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce44284",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VietnameseTextDataset(texts=sentences, tokenizer=tokenizer, max_length=256, stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afe687",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_texts_from_folder(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = prepare_vietnamese_dataset(data_folder=\"train_data_1\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 2:  # Only show first 2 batches\n",
    "        break\n",
    "        \n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Target IDs shape: {batch['target_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show first sequence in batch\n",
    "    input_seq = batch['input_ids'][2]\n",
    "    target_seq = batch['target_ids'][2]\n",
    "    \n",
    "    print(f\"  Sample input:  {input_seq[:50].tolist()}...\")\n",
    "    print(f\"  Sample target: {target_seq[:50].tolist()}...\")\n",
    "    \n",
    "    # Decode sample\n",
    "    decoded_input = tokenizer.decode(input_seq.tolist())\n",
    "    decoded_target = tokenizer.decode(target_seq.tolist())\n",
    "    \n",
    "    print(f\"  Decoded input:  '{decoded_input[:50]}...'\")\n",
    "    print(f\"  Decoded target: '{decoded_target[:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data configuration\n",
    "    'data_folder': 'data1',\n",
    "    'tokenizer_file': 'vietnamese_bpe_tokenizer.json',\n",
    "    'vocab_size': 25000,\n",
    "    'max_seq_len': 512,\n",
    "    'train_split': 0.8,\n",
    "    \n",
    "    # Model configuration\n",
    "    'd_model': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'd_ff': 3072,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Training configuration\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 3e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 20,\n",
    "    'warmup_steps': 5000,\n",
    "    'device': 'auto',  # 'cuda', 'cpu', or 'auto'\n",
    "    \n",
    "    # Generation configuration\n",
    "    'temperature': 0.8,\n",
    "    'top_k': 10,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 100,\n",
    "    \n",
    "    # Save configuration\n",
    "    'model_save_path': 'vietnamese_transformer_best.pt',\n",
    "    'config_save_path': 'training_config.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import VietnameseTransformer\n",
    "\n",
    "model = VietnameseTransformer(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_len=config['max_seq_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_token_id=tokenizer.token_to_id(\"[PAD]\")\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "print(f\"‚úÖ Model created successfully!\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import VietnameseTrainer\n",
    "from src.helpers import test_generation\n",
    "# Step 3: Initialize trainer\n",
    "print(f\"\\n{'='*20} STEP 3: TRAINING SETUP {'='*20}\")\n",
    "\n",
    "trainer = VietnameseTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Trainer initialized!\")\n",
    "print(f\"   Device: {trainer.device}\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "\n",
    "# Test initial generation (before training)\n",
    "print(f\"\\n{'='*20} INITIAL GENERATION TEST {'='*20}\")\n",
    "print(\"Testing generation before training (should be random):\")\n",
    "test_generation(model, tokenizer, trainer.device, [\"th∆° l·ª•c b√°t: ai ∆°i xa b·∫øn qu√™ h∆∞∆°ng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "print(f\"\\n{'='*20} STEP 4: TRAINING {'='*20}\")\n",
    "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "print(\"Press Ctrl+C to stop training early\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train(\n",
    "        num_epochs=config['num_epochs'],\n",
    "        save_path=config['model_save_path']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚èπÔ∏è  Training interrupted by user\")\n",
    "    print(\"Saving current model state...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'train_losses': trainer.train_losses,\n",
    "        'val_losses': trainer.val_losses,\n",
    "        # 'tokenizer': tokenizer\n",
    "    }, 'vietnamese_transformer_interrupted.pt')\n",
    "    print(\"Model saved as 'vietnamese_transformer_interrupted.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26652626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Final generation test\n",
    "print(f\"\\n{'='*20} STEP 6: FINAL GENERATION TEST {'='*20}\")\n",
    "print(config['model_save_path'])\n",
    "# Load best model for testing\n",
    "if os.path.exists(config['model_save_path']):\n",
    "    # Load the entire checkpoint dictionary without the `weights_only` flag\n",
    "    checkpoint = torch.load(config['model_save_path'], map_location=trainer.device, weights_only=True)\n",
    "\n",
    "    # Load only the model's weights from the loaded dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"‚úÖ Loaded best model for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer, device, test_cases=None):\n",
    "    \"\"\"Test text generation with various examples\"\"\"\n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            \"Truy·ªán Ki·ªÅu l√†\",\n",
    "        ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ TESTING TEXT GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, prompt in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        print(f\"Input: '{prompt}'\")\n",
    "        \n",
    "        # Encode input\n",
    "        input_ids = torch.tensor(\n",
    "            [tokenizer.encode(prompt, add_special_tokens=False).ids],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Generate with different settings\n",
    "        generation_configs = [\n",
    "            {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'max_new_tokens': 15, 'name': 'Balanced'},\n",
    "            {'temperature': 1.0, 'top_k': 20, 'top_p': 0.8, 'max_new_tokens': 15, 'name': 'Creative'},\n",
    "            {'temperature': 0.0, 'top_k': 5, 'top_p': 1.0, 'max_new_tokens': 15, 'name': 'Conservative'}\n",
    "        ]\n",
    "        \n",
    "        for config in generation_configs:\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    temperature=config['temperature'],\n",
    "                    top_k=config['top_k'],\n",
    "                    top_p=config['top_p'],\n",
    "                    max_new_tokens=config['max_new_tokens'],\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "            print(f\"  {config['name']}: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation(model, tokenizer, device=\"cuda\", test_cases = [\"S√≥ng ƒë∆∞·ª£c s√°ng t√°c b·ªüi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
