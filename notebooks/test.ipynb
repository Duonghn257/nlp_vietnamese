{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()   # split on whitespace\n",
    "trainer = BpeTrainer(vocab_size=10000,\n",
    "                     special_tokens=[\"[PAD]\", \"[UNK]\", \"[EOS]\"])\n",
    "files = [\"data1/truyen_kieu.txt\", \"\"]  # your raw text files\n",
    "tokenizer.train(files, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ceaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b143aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save(\"vietnamese_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f811d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create constant positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # shape (1, max_len, d_model)\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)  # self-attention\n",
    "        x = x + self.dropout(attn_out)       # residual + dropout\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)         # residual + dropout\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=4, d_ff=1024, n_layers=4, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=max_seq_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        bsz, seq_len = x.size()\n",
    "        # embed tokens and add positional encoding\n",
    "        tok_emb = self.token_emb(x)                     # (bsz, seq_len, d_model)\n",
    "        x = self.pos_enc(tok_emb)\n",
    "        # Prepare for MultiheadAttention: needs (seq_len, bsz, d_model)\n",
    "        x = x.transpose(0, 1)\n",
    "        # Causal mask to ensure autoregressive (upper-triangular -inf)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "        mask = mask.masked_fill(mask, float('-inf'))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "        x = x.transpose(0, 1)  # back to (bsz, seq_len, d_model)\n",
    "        logits = self.head(x)  # (bsz, seq_len, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Hyperparameters (tweak)\n",
    "# -------------------------\n",
    "VOCAB_SIZE = None  # set after loading tokenizer\n",
    "SEQ_LEN = 512       # context length (adjust: 128-512)\n",
    "BATCH_SIZE = 32     # try 16-64 depending on GPU memory\n",
    "NUM_EPOCHS = 10\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 200\n",
    "MAX_GRAD_NORM = 1.0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "CHECKPOINT_EVERY = 500   # steps\n",
    "VALIDATION_SPLIT = 0.05  # small val set\n",
    "USE_FP16 = True          # mixed precision (autocast) if supported\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset utils\n",
    "# -------------------------\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Build dataset by concatenating all token ids and sampling contiguous windows.\n",
    "    This is memory-efficient for small datasets; for huge corpora you'd stream.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_files: List[str], tokenizer: Tokenizer, seq_len: int, stride: int = None):\n",
    "        \"\"\"\n",
    "        token_files: list of raw text file paths (UTF-8)\n",
    "        tokenizer: tokenizer object with .encode(text)->List[int]\n",
    "        seq_len: desired example length (model context)\n",
    "        stride: if None -> random sampling windows; else sliding window with stride\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # read + encode all files into one long list of token ids\n",
    "        ids = []\n",
    "        for fn in token_files:\n",
    "            text = open(fn, \"r\", encoding=\"utf-8\").read().strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            enc = tokenizer.encode(text, add_special_tokens=False)\n",
    "            # optionally add [EOS] after each file to separate lessons\n",
    "            eos_id = tokenizer.token_to_id(\"[EOS]\") if hasattr(tokenizer, \"token_to_id\") else tokenizer.token_to_id(\"[EOS]\")\n",
    "            if eos_id is None:\n",
    "                raise ValueError(\"Tokenizer must have [EOS]\")\n",
    "            \n",
    "            # print(\"ENC =>>>>>>>>>\", type(enc), enc.ids)\n",
    "            # print(\"ENC TOKENS =>>>>>>>>>\", type(enc), tokenizer.decode(enc.ids))\n",
    "            # print(\"ENC ID =>>>>>>>>>\", type(eos_id), eos_id)\n",
    "            enc = enc.ids + [eos_id]\n",
    "            ids.extend(enc)\n",
    "        self.ids = ids\n",
    "\n",
    "        # create start indices for windows\n",
    "        if stride is None:\n",
    "            # we will sample random windows on the fly\n",
    "            self.starts = None\n",
    "        else:\n",
    "            self.starts = list(range(0, max(1, len(ids) - seq_len + 1), stride))\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.starts is None:\n",
    "            # define a large epoch size, sample randomly\n",
    "            return max(1000, len(self.ids) // self.seq_len)\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.starts is None:\n",
    "            # random crop\n",
    "            if len(self.ids) <= self.seq_len:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = random.randint(0, len(self.ids) - self.seq_len)\n",
    "        else:\n",
    "            start = self.starts[idx]\n",
    "        window = self.ids[start:start + self.seq_len]\n",
    "        # pad if needed\n",
    "        if len(window) < self.seq_len:\n",
    "            pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "            window = window + [pad_id] * (self.seq_len - len(window))\n",
    "        x = torch.tensor(window[:-1], dtype=torch.long)  # input tokens (seq_len-1)\n",
    "        y = torch.tensor(window[1:], dtype=torch.long)   # targets (shifted)\n",
    "        print(\"Source  =>>>>>>>>>>>>>\", self.tokenizer.decode(x.tolist()))\n",
    "        print(x.tolist())\n",
    "        print(\"Target  =>>>>>>>>>>>>>\", self.tokenizer.decode(y.tolist()))\n",
    "        print(y.tolist())\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d069b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(\"./data\", \"*.txt\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"vietnamese_bpe_tokenizer.json\")\n",
    "data = TextDataset(token_files=files, tokenizer=tokenizer, seq_len=SEQ_LEN)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068584e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7912826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.stack(ys, dim=0)\n",
    "    return x, y\n",
    "\n",
    "# -------------------------\n",
    "# Model / Tokenizer loader\n",
    "# -------------------------\n",
    "# You must have tokenizer and GPTModel in scope (from previous code).\n",
    "# Example:\n",
    "# from tokenizers import Tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"vietnamese_bpe_tokenizer.json\")\n",
    "# VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "#\n",
    "# from model import GPTModel\n",
    "# model = GPTModel(VOCAB_SIZE, d_model=256, n_heads=4, d_ff=1024, n_layers=4, max_seq_len=SEQ_LEN)\n",
    "# model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Prepare dataset and dataloaders\n",
    "# -------------------------\n",
    "def prepare_dataloaders(all_files, tokenizer, seq_len, batch_size, val_split=VALIDATION_SPLIT):\n",
    "    random.shuffle(all_files)\n",
    "    n_val = max(1, int(len(all_files) * val_split))\n",
    "    train_files = all_files[n_val:]\n",
    "    val_files = all_files[:n_val]\n",
    "\n",
    "    train_ds = TextDataset(train_files, tokenizer, seq_len)\n",
    "    val_ds = TextDataset(val_files, tokenizer, seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=2)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# -------------------------\n",
    "# Training helpers\n",
    "# -------------------------\n",
    "def save_checkpoint(state, path):\n",
    "    torch.save(state, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, scheduler=None):\n",
    "    ckpt = torch.load(path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    if optimizer and \"optimizer_state\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    if scheduler and \"scheduler_state\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    step = ckpt.get(\"step\", 0)\n",
    "    return step\n",
    "\n",
    "# -------------------------\n",
    "# Training loop\n",
    "# -------------------------\n",
    "def train(model, tokenizer, data_dir, seq_len=SEQ_LEN, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS):\n",
    "    global VOCAB_SIZE\n",
    "    VOCAB_SIZE = tokenizer.get_vocab_size() if hasattr(tokenizer, \"get_vocab_size\") else len(tokenizer.get_vocab())\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # prepare files\n",
    "    files = glob(os.path.join(data_dir, \"*.txt\"))\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(\"No .txt files found in data_dir\")\n",
    "\n",
    "    train_loader, val_loader = prepare_dataloaders(files, tokenizer, seq_len, batch_size)\n",
    "\n",
    "    # optimizer + scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    # simple linear warmup then decay\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    def lr_lambda(step):\n",
    "        if step < WARMUP_STEPS:\n",
    "            return float(step) / float(max(1, WARMUP_STEPS))\n",
    "        return max(0.0, float(total_steps - step) / float(max(1, total_steps - WARMUP_STEPS)))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\"))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16 and DEVICE.startswith(\"cuda\"))\n",
    "\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        running_loss = 0.0\n",
    "        for batch in pbar:\n",
    "            x, y = batch  # x: (B, seq_len-1), y: (B, seq_len-1)\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=USE_FP16 and DEVICE.startswith(\"cuda\")):\n",
    "                logits = model(x)  # (B, L, V)\n",
    "                # flatten for CE\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix({'loss': f\"{running_loss/global_step:.4f}\", 'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "            # checkpoint\n",
    "            if global_step % CHECKPOINT_EVERY == 0:\n",
    "                ckpt_path = os.path.join(SAVE_DIR, f\"ckpt_step{global_step}.pt\")\n",
    "                save_checkpoint({\n",
    "                    \"step\": global_step,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scheduler_state\": scheduler.state_dict(),\n",
    "                    \"tokenizer\": None\n",
    "                }, ckpt_path)\n",
    "                print(\"Saved\", ckpt_path)\n",
    "\n",
    "        # end epoch -> validate\n",
    "        val_loss = evaluate(model, val_loader, criterion, tokenizer)\n",
    "        print(f\"Epoch {epoch+1} finished. Validation loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_path = os.path.join(SAVE_DIR, \"best_model.pt\")\n",
    "            save_checkpoint({\n",
    "                \"step\": global_step,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"tokenizer\": None\n",
    "            }, best_path)\n",
    "            print(\"Saved best model to\", best_path)\n",
    "\n",
    "    print(\"Training finished. Best val loss:\", best_val_loss)\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate(model, val_loader, criterion, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            # scale by number of tokens\n",
    "            total_loss += loss.item() * x.size(0) * x.size(1)\n",
    "            total_tokens += x.size(0) * x.size(1)\n",
    "    return total_loss / total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"vietnamese_bpe_tokenizer.json\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "print(VOCAB_SIZE)\n",
    "# import your model class (previous message)\n",
    "# from model import GPTModel\n",
    "# model = GPTModel(VOCAB_SIZE, d_model=256, n_heads=4, d_ff=1024, n_layers=4, max_seq_len=SEQ_LEN)\n",
    "# train(model, tokenizer, data_dir=\"data\", seq_len=SEQ_LEN, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import VietnameseTransformer\n",
    "vocab_size = VOCAB_SIZE\n",
    "model = VietnameseTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    d_ff=2048,\n",
    "    max_seq_len=128,\n",
    "    dropout=0.1\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"Truyện Kiều được viết\"\n",
    "input_ids = torch.tensor(\n",
    "    [tokenizer.encode(sample_input, add_special_tokens=False).ids],\n",
    "    # device=\"auto\"\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "generated_tokens = input_ids.clone()\n",
    "logits, loss = model(generated_tokens, return_loss=True)\n",
    "print(loss)\n",
    "# Get logits for last position\n",
    "next_token_logits = logits[:, -1, :] / 1.0\n",
    "top_k = 10\n",
    "top_p = 0.9\n",
    "do_sample = True\n",
    "\n",
    "# logits, loss = model(generated_tokens)\n",
    "# print(loss)\n",
    "# Apply top-k filtering\n",
    "if top_k > 0:\n",
    "    top_k = min(top_k, next_token_logits.size(-1))\n",
    "    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "    next_token_logits = torch.full_like(next_token_logits, -float('inf'))\n",
    "    next_token_logits.scatter_(-1, top_k_indices, top_k_logits)\n",
    "\n",
    "# Apply top-p (nucleus) filtering\n",
    "if top_p < 1.0:\n",
    "    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Keep at least one token\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    next_token_logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "# Sample or greedy decode\n",
    "if do_sample:\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "else:\n",
    "    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "generated_tokens = torch.cat([generated_tokens, next_tokens], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([469])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
