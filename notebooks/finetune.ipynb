{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c83b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/duongnh/abc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import DatasetDict\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Directory to save the fine-tuned model and tokenizer\n",
    "output_dir = \"./qwen-vietnamese-poetry-lora\"\n",
    "\n",
    "# Load the Vietnamese poetry dataset from HuggingFace Hub\n",
    "dataset = load_dataset(\"truongpdd/vietnamese_poetry\")\n",
    "\n",
    "# Specify the base model to fine-tune\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the tokenizer for the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 31107.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 144785.73 examples/s]\n",
      "Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 111238.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def create_conversation(example):\n",
    "    \"\"\"\n",
    "    Convert a raw poetry example into a structured conversation for instruction tuning.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary with a 'text' field containing the poem.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of message dicts representing a conversation with system, user, and assistant roles.\n",
    "    \"\"\"\n",
    "    # Split the text into words and create a prompt/completion split\n",
    "    words = example['text'].split()\n",
    "    prompt = \" \".join(words[:50])\n",
    "    completion = \" \".join(words[50:])\n",
    "\n",
    "    # Construct the conversation with system, user, and assistant messages\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Bạn là một chuyên gia về văn học Việt Nam. \"\n",
    "                \"Hãy trả lời các câu hỏi về tác phẩm và tác giả một cách chính xác và sâu sắc.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Hãy viết tiếp đoạn văn sau:\\n{prompt}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": completion\n",
    "        }\n",
    "    ]\n",
    "    return conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833e0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_template(example):\n",
    "    \"\"\"\n",
    "    Apply the chat template to each example, formatting it for model input.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dataset example.\n",
    "\n",
    "    Returns:\n",
    "        dict: The example with an added 'formatted_text' field.\n",
    "    \"\"\"\n",
    "    conversation = create_conversation(example)\n",
    "    # Use the tokenizer's chat template to format the conversation as a string\n",
    "    example['formatted_text'] = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    return example\n",
    "\n",
    "# Format the entire dataset using the chat template\n",
    "formatted_dataset = dataset.map(apply_template)\n",
    "\n",
    "# Split the dataset: 10% for testing, then further split into train/eval\n",
    "tmp_dataset = formatted_dataset['train'].train_test_split(test_size=0.1)[\"test\"]\n",
    "train_dataset = tmp_dataset.train_test_split(test_size=0.1)[\"train\"]\n",
    "eval_dataset = tmp_dataset.train_test_split(test_size=0.1)[\"test\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the tokenizer and set padding tokens and side\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure 4-bit quantization for memory-efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the base model with quantization and device mapping\n",
    "# FIXED: Changed device_map to \"auto\" instead of \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Changed from \"cuda\" to \"auto\"\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Disable cache during training for compatibility with LoRA\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Prepare the model for k-bit (quantized) training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # LoRA rank\n",
    "    lora_alpha=4,  # LoRA scaling factor\n",
    "    target_modules=[  # Modules to apply LoRA to (specific to Qwen2.5)\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # No bias adaptation\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Changed from 0.2 to fixed number\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",  # Changed from \"epoch\" to \"steps\"\n",
    "    save_steps=100,  # Changed from 0.2 to fixed number\n",
    "    max_steps=-1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,  # Changed from False to True for better performance\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    run_name=\"qwen-vietnamese-poetry-finetune\",\n",
    "    remove_unused_columns=False,  # Added this to prevent column removal issues\n",
    "    dataloader_pin_memory=False,  # Added to prevent memory issues\n",
    ")\n",
    "\n",
    "# Data collator for language modeling (no masked LM, left padding to multiple of 8)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90adf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Extract the formatted text for each example.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dataset example with 'formatted_text'.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted text for training.\n",
    "    \"\"\"\n",
    "    return example['formatted_text']\n",
    "\n",
    "# Initialize the SFTTrainer for supervised fine-tuning\n",
    "# FIXED: Removed peft_config from SFTTrainer since model already has LoRA applied\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_func,\n",
    "    # max_seq_length=512,  # Added max sequence length\n",
    "    packing=False,  # Disabled packing to avoid issues\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea666cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer to disk\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Training completed! Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab1241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0ba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb22705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
