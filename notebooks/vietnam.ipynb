{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4199595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.dataset import VietnameseTextDataset, prepare_vietnamese_dataset, load_texts_from_folder\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from glob import glob\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9496237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()   # split on whitespace\n",
    "trainer = BpeTrainer(vocab_size=1000,\n",
    "                     special_tokens=[\"[PAD]\", \"[UNK]\", \"[EOS]\"])\n",
    "files = glob(os.path.join(\"./data\", \"*.txt\"))\n",
    "files\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.save(\"vietnamese_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"vietnamese_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a872a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(\"./data\", \"*.txt\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for data_file in files:\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "        sentences.append(raw_text)\n",
    "# sentences[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import VietnamesePreprocessor\n",
    "import random\n",
    "preprocessor = VietnamesePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = load_texts_from_folder(\"data\")\n",
    "all_sentences = []\n",
    "for text in raw_texts:\n",
    "    cleaned_text = preprocessor.clean_text(text)\n",
    "    sentences_from_file = preprocessor.segment_sentences(cleaned_text)\n",
    "    all_sentences.extend(sentences_from_file) # Use extend to add all sentences to one list\n",
    "\n",
    "train_split = 0.8\n",
    "random.shuffle(all_sentences)\n",
    "split_idx = int(len(all_sentences) * train_split)\n",
    "train_sentences = all_sentences[:split_idx]\n",
    "val_sentences = all_sentences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb69998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7aa3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = VietnameseTextDataset(texts=train_sentences, tokenizer=tokenizer, max_length=128, stride=64)\n",
    "train_data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce44284",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VietnameseTextDataset(texts=sentences, tokenizer=tokenizer, max_length=128, stride=94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afe687",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_texts_from_folder(\"data\")\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, _ = prepare_vietnamese_dataset(data_folder=\"data\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 2:  # Only show first 2 batches\n",
    "        break\n",
    "        \n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Target IDs shape: {batch['target_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show first sequence in batch\n",
    "    input_seq = batch['input_ids'][2]\n",
    "    target_seq = batch['target_ids'][2]\n",
    "    \n",
    "    print(f\"  Sample input:  {input_seq[:50].tolist()}...\")\n",
    "    print(f\"  Sample target: {target_seq[:50].tolist()}...\")\n",
    "    \n",
    "    # Decode sample\n",
    "    decoded_input = tokenizer.decode(input_seq.tolist())\n",
    "    decoded_target = tokenizer.decode(target_seq.tolist())\n",
    "    \n",
    "    print(f\"  Decoded input:  '{decoded_input[:50]}...'\")\n",
    "    print(f\"  Decoded target: '{decoded_target[:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data configuration\n",
    "    'data_folder': 'data1',\n",
    "    'tokenizer_file': 'vietnamese_bpe_tokenizer.json',\n",
    "    'vocab_size': 5000,\n",
    "    'max_seq_len': 128,\n",
    "    'train_split': 0.8,\n",
    "    \n",
    "    # Model configuration\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 6,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Training configuration\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 50,\n",
    "    'warmup_steps': 1000,\n",
    "    'device': 'auto',  # 'cuda', 'cpu', or 'auto'\n",
    "    \n",
    "    # Generation configuration\n",
    "    'temperature': 0.8,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.9,\n",
    "    'max_new_tokens': 50,\n",
    "    \n",
    "    # Save configuration\n",
    "    'model_save_path': 'vietnamese_transformer_best.pt',\n",
    "    'config_save_path': 'training_config.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import VietnameseTransformer\n",
    "\n",
    "model = VietnameseTransformer(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_layers=config['n_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_len=config['max_seq_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_token_id=tokenizer.token_to_id(\"[PAD]\")\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "print(f\"‚úÖ Model created successfully!\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import VietnameseTrainer\n",
    "from trainer import test_generation\n",
    "# Step 3: Initialize trainer\n",
    "print(f\"\\n{'='*20} STEP 3: TRAINING SETUP {'='*20}\")\n",
    "\n",
    "trainer = VietnameseTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    device=config['device']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Trainer initialized!\")\n",
    "print(f\"   Device: {trainer.device}\")\n",
    "print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "\n",
    "# Test initial generation (before training)\n",
    "print(f\"\\n{'='*20} INITIAL GENERATION TEST {'='*20}\")\n",
    "print(\"Testing generation before training (should be random):\")\n",
    "test_generation(model, tokenizer, trainer.device, [\"Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "print(f\"\\n{'='*20} STEP 4: TRAINING {'='*20}\")\n",
    "print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "print(\"Press Ctrl+C to stop training early\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train(\n",
    "        num_epochs=config['num_epochs'],\n",
    "        save_path=config['model_save_path']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚èπÔ∏è  Training interrupted by user\")\n",
    "    print(\"Saving current model state...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'train_losses': trainer.train_losses,\n",
    "        'val_losses': trainer.val_losses,\n",
    "        'tokenizer': tokenizer\n",
    "    }, 'vietnamese_transformer_interrupted.pt')\n",
    "    print(\"Model saved as 'vietnamese_transformer_interrupted.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26652626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Final generation test\n",
    "print(f\"\\n{'='*20} STEP 6: FINAL GENERATION TEST {'='*20}\")\n",
    "print(config['model_save_path'])\n",
    "# Load best model for testing\n",
    "if os.path.exists(config['model_save_path']):\n",
    "    checkpoint = torch.load(config['model_save_path'], map_location=trainer.device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"‚úÖ Loaded best model for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer, device, test_cases=None):\n",
    "    \"\"\"Test text generation with various examples\"\"\"\n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            \"Truy·ªán Ki·ªÅu l√†\",\n",
    "        ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ TESTING TEXT GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, prompt in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        print(f\"Input: '{prompt}'\")\n",
    "        \n",
    "        # Encode input\n",
    "        input_ids = torch.tensor(\n",
    "            [tokenizer.encode(prompt, add_special_tokens=False).ids],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Generate with different settings\n",
    "        generation_configs = [\n",
    "            {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'max_new_tokens': 15, 'name': 'Balanced'},\n",
    "            {'temperature': 1.0, 'top_k': 20, 'top_p': 0.8, 'max_new_tokens': 15, 'name': 'Creative'},\n",
    "            {'temperature': 0.0, 'top_k': 5, 'top_p': 1.0, 'max_new_tokens': 15, 'name': 'Conservative'}\n",
    "        ]\n",
    "        \n",
    "        for config in generation_configs:\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    temperature=config['temperature'],\n",
    "                    top_k=config['top_k'],\n",
    "                    top_p=config['top_p'],\n",
    "                    max_new_tokens=config['max_new_tokens'],\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
    "            print(f\"  {config['name']}: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation(model, tokenizer, device=\"cuda\", test_cases = [\"Truy·ªán Ki·ªÅu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
