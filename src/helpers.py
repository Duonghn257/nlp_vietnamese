#!/usr/bin/env python3
"""
Complete training script for Vietnamese Text Generation using Decoder-Only Transformer
Usage: python train_vietnamese_transformer.py
"""

import torch
import torch.nn as nn
import os
import argparse
import json
from pathlib import Path
import matplotlib.pyplot as plt

# Import your modules (make sure these files are in the same directory)
from src.tokenizer import VietnamesePreprocessor, VietnameseTokenizer
from src.dataset import prepare_vietnamese_dataset
from src.trainer import VietnameseTrainer
from src.model import VietnameseTransformer


def setup_training_config():
    """Setup training configuration"""
    config = {
        # Data configuration
        "data_file": "truyen_kieu.txt",
        "tokenizer_file": "vietnamese_tokenizer.json",
        "vocab_size": 5000,
        "max_seq_len": 128,
        "train_split": 0.8,
        # Model configuration
        "d_model": 512,
        "n_heads": 8,
        "n_layers": 6,
        "d_ff": 2048,
        "dropout": 0.1,
        # Training configuration
        "batch_size": 16,
        "learning_rate": 1e-4,
        "weight_decay": 0.01,
        "num_epochs": 50,
        "warmup_steps": 1000,
        "device": "auto",  # 'cuda', 'cpu', or 'auto'
        # Generation configuration
        "temperature": 0.8,
        "top_k": 50,
        "top_p": 0.9,
        "max_new_tokens": 50,
        # Save configuration
        "model_save_path": "vietnamese_transformer_best.pt",
        "config_save_path": "training_config.json",
    }
    return config


def create_sample_data(file_path: str):
    """Create sample Vietnamese literature data if file doesn't exist"""
    sample_data = (
        """
    TrƒÉm nƒÉm trong c√µi ng∆∞·ªùi ta,
    Ch·ªØ t√†i ch·ªØ m·ªánh kh√©o l√† gh√©t nhau.
    Tr·∫£i qua m·ªôt cu·ªôc b·ªÉ d√¢u,
    Nh·ªØng ƒëi·ªÅu tr√¥ng th·∫•y m√† ƒëau ƒë·ªõn l√≤ng.
    L·∫° g√¨ b·ªâ s·∫Øc t∆∞ phong,
    Tr·ªùi xanh quen th√≥i m√° h·ªìng ƒë√°nh ghen.
    C≈©ng ƒë√†nh r·∫±ng s·ªë ki·∫øp en,
    V√¨ ch∆∞ng n√†ng s·∫Øc n√™n th√™m n√†ng t√†i.

    Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt b·ªüi Nguy·ªÖn Du v√†o th·∫ø k·ª∑ XIX.
    T√°c ph·∫©m n√†y ƒë∆∞·ª£c coi l√† ki·ªát t√°c c·ªßa vƒÉn h·ªçc Vi·ªát Nam.
    C√¢u chuy·ªán k·ªÉ v·ªÅ s·ªë ph·∫≠n c·ªßa Th√∫y Ki·ªÅu, m·ªôt c√¥ g√°i t√†i s·∫Øc v∆∞·ª£t tr·ªôi.
    Qua nhi·ªÅu thƒÉng tr·∫ßm trong cu·ªôc ƒë·ªùi, cu·ªëi c√πng n√†ng ƒë∆∞·ª£c ƒëo√†n t·ª• v·ªõi gia ƒë√¨nh.

    VƒÉn h·ªçc Vi·ªát Nam c√≥ nhi·ªÅu t√°c ph·∫©m n·ªïi ti·∫øng kh√°c.
    S·ªë ƒê·ªè c·ªßa V≈© Tr·ªçng Ph·ª•ng l√† m·ªôt ti·ªÉu thuy·∫øt hi·ªán th·ª±c xu·∫•t s·∫Øc.
    Kim L√¢n n·ªïi ti·∫øng v·ªõi nh·ªØng truy·ªán ng·∫Øn v·ªÅ cu·ªôc s·ªëng n√¥ng th√¥n.
    Nguy·ªÖn Tu√¢n ƒë∆∞·ª£c bi·∫øt ƒë·∫øn v·ªõi vƒÉn xu√¥i mi√™u t·∫£ thi√™n nhi√™n tuy·ªát ƒë·∫πp.

    Th∆° ca c·ªï ƒëi·ªÉn Vi·ªát Nam th∆∞·ªùng s·ª≠ d·ª•ng th·ªÉ l·ª•c b√°t, th·∫•t ng√¥n t·ª© tuy·ªát.
    Truy·ªÅn th·ªëng vƒÉn h·ªçc d√¢n gian r·∫•t phong ph√∫ v·ªõi c√°c c√¢u chuy·ªán c·ªï t√≠ch.
    T·∫•m C√°m, Th·∫°ch Sanh, S∆°n Tinh Th·ªßy Tinh l√† nh·ªØng truy·ªán n·ªïi ti·∫øng.
    C√°c c√¢u ca dao, t·ª•c ng·ªØ c≈©ng th·ªÉ hi·ªán tri·∫øt l√Ω s·ªëng s√¢u s·∫Øc.

    H·ªì Ch√≠ Minh c≈©ng c√≥ nh·ªØng b√†i th∆° n·ªïi ti·∫øng vi·∫øt trong t√π.
    Nh·∫≠t k√Ω trong t√π th·ªÉ hi·ªán tinh th·∫ßn ki√™n c∆∞·ªùng c·ªßa ng∆∞·ªùi c·ªông s·∫£n.
    VƒÉn h·ªçc hi·ªán ƒë·∫°i Vi·ªát Nam ph√°t tri·ªÉn m·∫°nh m·∫Ω t·ª´ ƒë·∫ßu th·∫ø k·ª∑ XX.
    Nhi·ªÅu t√°c gi·∫£ tr·∫ª ƒë√£ g√≥p ph·∫ßn l√†m gi√†u kho t√†ng vƒÉn h·ªçc d√¢n t·ªôc.

    Ng√¥n ng·ªØ Vi·ªát Nam c√≥ ƒë·∫∑c ƒëi·ªÉm l√† ƒë∆°n √¢m ti·∫øt v√† c√≥ thanh ƒëi·ªáu.
    M·ªói ti·∫øng c√≥ th·ªÉ mang nhi·ªÅu nghƒ©a kh√°c nhau t√πy theo thanh ƒëi·ªáu.
    ƒêi·ªÅu n√†y t·∫°o n√™n s·ª± phong ph√∫ v√† ƒëa d·∫°ng trong c√°ch di·ªÖn ƒë·∫°t.
    VƒÉn h·ªçc Vi·ªát Nam khai th√°c tri·ªát ƒë·ªÉ v·∫ª ƒë·∫πp c·ªßa ng√¥n ng·ªØ n√†y.

    Truy·ªán Ki·ªÅu kh√¥ng ch·ªâ l√† t√°c ph·∫©m vƒÉn h·ªçc m√† c√≤n l√† b·ª©c tranh x√£ h·ªôi.
    N√≥ ph·∫£n √°nh nh·ªØng m√¢u thu·∫´n s√¢u s·∫Øc c·ªßa x√£ h·ªôi phong ki·∫øn.
    S·ªë ph·∫≠n con ng∆∞·ªùi b·ªã chi ph·ªëi b·ªüi ho√†n c·∫£nh x√£ h·ªôi.
    T√¨nh y√™u v√† l√≤ng hi·∫øu th·∫£o l√† nh·ªØng gi√° tr·ªã ƒë∆∞·ª£c t√¥n vinh.

    Ng√†y nay, vƒÉn h·ªçc Vi·ªát Nam ti·∫øp t·ª•c ph√°t tri·ªÉn v·ªõi nhi·ªÅu h√¨nh th·ª©c m·ªõi.
    Ti·ªÉu thuy·∫øt, truy·ªán ng·∫Øn, th∆°, k·ªãch ƒë·ªÅu c√≥ nh·ªØng t√°c ph·∫©m xu·∫•t s·∫Øc.
    C√°c nh√† vƒÉn tr·∫ª mang ƒë·∫øn l√†n gi√≥ m·ªõi cho n·ªÅn vƒÉn h·ªçc.
    VƒÉn h·ªçc Vi·ªát Nam ng√†y c√†ng ƒë∆∞·ª£c qu·ªëc t·∫ø quan t√¢m v√† ƒë√°nh gi√° cao.
    """
        * 3
    )  # Repeat to have more training data

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(sample_data)
    print(f"‚úÖ Created sample data file: {file_path}")


def plot_training_history(train_losses, val_losses, save_path="training_history.png"):
    """Plot and save training history"""
    plt.figure(figsize=(10, 6))

    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, "b-", label="Training Loss", linewidth=2)
    plt.plot(epochs, val_losses, "r-", label="Validation Loss", linewidth=2)

    plt.title("Vietnamese Transformer Training History", fontsize=16, fontweight="bold")
    plt.xlabel("Epoch", fontsize=12)
    plt.ylabel("Loss", fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # Add annotations
    min_val_loss_epoch = val_losses.index(min(val_losses)) + 1
    plt.annotate(
        f"Best Val Loss: {min(val_losses):.4f}\nEpoch: {min_val_loss_epoch}",
        xy=(min_val_loss_epoch, min(val_losses)),
        xytext=(min_val_loss_epoch + len(val_losses) * 0.1, min(val_losses) + 0.1),
        arrowprops=dict(arrowstyle="->", color="red"),
        fontsize=10,
        ha="left",
    )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"üìä Training history saved to: {save_path}")


def test_generation(
    model, tokenizer, device, test_cases=None, max_new_tokens: int = 20
):
    """Test text generation with various examples"""
    if test_cases is None:
        test_cases = [
            "Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt",
            "VƒÉn h·ªçc Vi·ªát Nam",
            "Nguy·ªÖn Du l√†",
            "Th√∫y Ki·ªÅu",
            "T√°c ph·∫©m n√†y",
        ]

    print("\n" + "=" * 60)
    print("üéØ TESTING TEXT GENERATION")
    print("=" * 60)

    model.eval()

    for i, prompt in enumerate(test_cases, 1):
        print(f"\n--- Test {i} ---")
        print(f"Input: '{prompt}'")

        # Encode input
        input_ids = torch.tensor(
            [tokenizer.encode(prompt, add_special_tokens=False).ids], device=device
        )

        # Generate with different settings
        generation_configs = [
            {
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.9,
                "max_new_tokens": max_new_tokens,
                "name": "Balanced",
            },
            {
                "temperature": 1.0,
                "top_k": 20,
                "top_p": 0.8,
                "max_new_tokens": max_new_tokens,
                "name": "Creative",
            },
            {
                "temperature": 0.3,
                "top_k": 10,
                "top_p": 1.0,
                "max_new_tokens": max_new_tokens,
                "name": "Conservative",
            },
        ]

        for config in generation_configs:
            with torch.no_grad():
                generated = model.generate(
                    input_ids,
                    temperature=config["temperature"],
                    top_k=config["top_k"],
                    top_p=config["top_p"],
                    max_new_tokens=config["max_new_tokens"],
                    do_sample=True,
                )

            generated_text = tokenizer.decode(generated[0].cpu().tolist())
            print(f"  {config['name']}: '{generated_text}'")


def save_model_and_config(model, tokenizer, config, model_path, config_path):
    """Save the trained model and configuration"""
    # Save model state
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "model_config": {
                "vocab_size": config["vocab_size"],
                "d_model": config["d_model"],
                "n_heads": config["n_heads"],
                "n_layers": config["n_layers"],
                "d_ff": config["d_ff"],
                "max_seq_len": config["max_seq_len"],
                "dropout": config["dropout"],
            },
            "tokenizer_file": config["tokenizer_file"],
        },
        model_path,
    )

    # Save configuration
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"üíæ Model saved to: {model_path}")
    print(f"‚öôÔ∏è  Configuration saved to: {config_path}")


def load_model_and_tokenizer(model_path, tokenizer_path):
    """Load trained model and tokenizer"""
    # Load model checkpoint
    checkpoint = torch.load(model_path, map_location="cpu")
    model_config = checkpoint["model_config"]

    # Create model
    model = VietnameseTransformer(**model_config)
    model.load_state_dict(checkpoint["model_state_dict"])

    # Load tokenizer
    tokenizer = VietnameseTokenizer()
    tokenizer.load(tokenizer_path)

    print(f"‚úÖ Model loaded from: {model_path}")
    print(f"‚úÖ Tokenizer loaded from: {tokenizer_path}")

    return model, tokenizer


def main():
    """Main training function"""
    print("üáªüá≥ Vietnamese Text Generation Training")
    print("=" * 50)

    # Load configuration
    config = setup_training_config()

    # Create data file if it doesn't exist
    if not os.path.exists(config["data_file"]):
        print(f"‚ö†Ô∏è  Data file not found: {config['data_file']}")
        print("Creating sample data...")
        create_sample_data(config["data_file"])

    print(f"üìä Training Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")

    # Step 1: Prepare dataset
    print(f"\n{'='*20} STEP 1: DATA PREPARATION {'='*20}")

    train_loader, val_loader, tokenizer = prepare_vietnamese_dataset(
        data_file=config["data_file"],
        tokenizer_file=config["tokenizer_file"],
        vocab_size=config["vocab_size"],
        max_length=config["max_seq_len"],
        train_split=config["train_split"],
        batch_size=config["batch_size"],
    )

    print(f"‚úÖ Dataset prepared successfully!")
    print(f"   Training batches: {len(train_loader)}")
    print(f"   Validation batches: {len(val_loader)}")
    print(f"   Vocabulary size: {len(tokenizer.vocab)}")

    # Step 2: Create model
    print(f"\n{'='*20} STEP 2: MODEL CREATION {'='*20}")

    model = VietnameseTransformer(
        vocab_size=len(tokenizer.vocab),
        d_model=config["d_model"],
        n_heads=config["n_heads"],
        n_layers=config["n_layers"],
        d_ff=config["d_ff"],
        max_seq_len=config["max_seq_len"],
        dropout=config["dropout"],
        pad_token_id=tokenizer.word_to_id[tokenizer.PAD_TOKEN],
    )

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"‚úÖ Model created successfully!")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")

    # Step 3: Initialize trainer
    print(f"\n{'='*20} STEP 3: TRAINING SETUP {'='*20}")

    trainer = VietnameseTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        tokenizer=tokenizer,
        lr=config["learning_rate"],
        weight_decay=config["weight_decay"],
        warmup_steps=config["warmup_steps"],
        device=config["device"],
    )

    print(f"‚úÖ Trainer initialized!")
    print(f"   Device: {trainer.device}")
    print(f"   Learning rate: {config['learning_rate']}")
    print(f"   Batch size: {config['batch_size']}")

    # Test initial generation (before training)
    print(f"\n{'='*20} INITIAL GENERATION TEST {'='*20}")
    print("Testing generation before training (should be random):")
    test_generation(model, tokenizer, trainer.device, ["Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt"])

    # Step 4: Train the model
    print(f"\n{'='*20} STEP 4: TRAINING {'='*20}")
    print(f"Starting training for {config['num_epochs']} epochs...")
    print("Press Ctrl+C to stop training early\n")

    try:
        trainer.train(
            num_epochs=config["num_epochs"], save_path=config["model_save_path"]
        )

        print(f"\nüéâ Training completed successfully!")

    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è  Training interrupted by user")
        print("Saving current model state...")
        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": trainer.optimizer.state_dict(),
                "train_losses": trainer.train_losses,
                "val_losses": trainer.val_losses,
            },
            "vietnamese_transformer_interrupted.pt",
        )
        print("Model saved as 'vietnamese_transformer_interrupted.pt'")

    # Step 5: Plot training history
    if trainer.train_losses and trainer.val_losses:
        print(f"\n{'='*20} STEP 5: TRAINING ANALYSIS {'='*20}")
        plot_training_history(trainer.train_losses, trainer.val_losses)

        print(f"Training Summary:")
        print(f"  Best validation loss: {min(trainer.val_losses):.4f}")
        print(f"  Final training loss: {trainer.train_losses[-1]:.4f}")
        print(f"  Final validation loss: {trainer.val_losses[-1]:.4f}")

    # Step 6: Final generation test
    print(f"\n{'='*20} STEP 6: FINAL GENERATION TEST {'='*20}")

    # Load best model for testing
    if os.path.exists(config["model_save_path"]):
        checkpoint = torch.load(config["model_save_path"], map_location=trainer.device)
        model.load_state_dict(checkpoint["model_state_dict"])
        print("‚úÖ Loaded best model for testing")

    # Test with multiple examples
    test_generation(model, tokenizer, trainer.device)

    # Step 7: Save final configuration
    save_model_and_config(
        model, tokenizer, config, config["model_save_path"], config["config_save_path"]
    )

    print(f"\n{'='*20} TRAINING COMPLETE {'='*20}")
    print("üéØ Your Vietnamese text generation model is ready!")
    print(f"üìÅ Model saved: {config['model_save_path']}")
    print(f"üìÅ Tokenizer: {config['tokenizer_file']}")
    print(f"üìÅ Config: {config['config_save_path']}")

    # Example usage instructions
    print(f"\n{'='*20} USAGE EXAMPLE {'='*20}")
    print("To use your trained model:")
    print("```python")
    print("# Load model and tokenizer")
    print(f"model, tokenizer = load_model_and_tokenizer(")
    print(f"    '{config['model_save_path']}', '{config['tokenizer_file']}')")
    print("")
    print("# Generate text")
    print("device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')")
    print("model.to(device)")
    print("model.eval()")
    print("")
    print("prompt = 'Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt'")
    print(
        "input_ids = torch.tensor([tokenizer.encode(prompt, add_special_tokens=False)], device=device)"
    )
    print("generated = model.generate(input_ids, max_new_tokens=20, temperature=0.8)")
    print("result = tokenizer.decode(generated[0].cpu().tolist())")
    print("print(result)")
    print("```")


if __name__ == "__main__":
    # Add argument parser for command line options
    parser = argparse.ArgumentParser(
        description="Train Vietnamese Text Generation Model"
    )
    parser.add_argument(
        "--data_file",
        type=str,
        default="data/truyen_kieu.txt",
        help="Path to the Vietnamese text data file",
    )
    parser.add_argument(
        "--vocab_size", type=int, default=5000, help="Vocabulary size for tokenizer"
    )
    parser.add_argument(
        "--batch_size", type=int, default=16, help="Training batch size"
    )
    parser.add_argument(
        "--epochs", type=int, default=50, help="Number of training epochs"
    )
    parser.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="Training device",
    )

    args = parser.parse_args()

    # Update config with command line arguments
    config = setup_training_config()
    config["data_file"] = args.data_file
    config["vocab_size"] = args.vocab_size
    config["batch_size"] = args.batch_size
    config["num_epochs"] = args.epochs
    config["learning_rate"] = args.lr
    config["device"] = args.device

    # Run training
    main()
