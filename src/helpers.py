#!/usr/bin/env python3
"""
Complete training script for Vietnamese Text Generation using Decoder-Only Transformer
Usage: python train_vietnamese_transformer.py
"""

import torch
import torch.nn as nn
import os
import argparse
import json
import yaml
from pathlib import Path
import matplotlib.pyplot as plt

# Import your modules (make sure these files are in the same directory)
from src.tokenizer import VietnamesePreprocessor, VietnameseTokenizer
from src.dataset import prepare_vietnamese_dataset
from src.trainer import VietnameseTrainer
from src.model import VietnameseTransformer

preprocessor = VietnamesePreprocessor()


def setup_training_config(config_path: str = "config.yaml"):
    """Setup training configuration from YAML file"""
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            yaml_config = yaml.safe_load(f)

        # Flatten the nested YAML structure for backward compatibility
        config = {}

        # Data configuration
        config.update(yaml_config.get("data", {}))

        # Model configuration
        config.update(yaml_config.get("model", {}))

        # Training configuration
        config.update(yaml_config.get("training", {}))

        # Generation configuration
        config.update(yaml_config.get("generation", {}))

        # Save configuration
        config.update(yaml_config.get("save", {}))

        # Convert string values to appropriate types
        config = convert_config_types(config)

        print(f"‚úÖ Configuration loaded from {config_path}")
        return config

    except FileNotFoundError:
        print(f"‚ö†Ô∏è  Config file {config_path} not found. Using default configuration.")
        return get_default_config()
    except yaml.YAMLError as e:
        print(f"‚ùå Error parsing YAML config file: {e}")
        return get_default_config()


def convert_config_types(config):
    """Convert string values in config to appropriate data types"""
    type_conversions = {
        # Data configuration
        "vocab_size": int,
        "max_seq_len": int,
        "train_split": float,
        # Model configuration
        "d_model": int,
        "n_heads": int,
        "n_layers": int,
        "d_ff": int,
        "dropout": float,
        # Training configuration
        "batch_size": int,
        "learning_rate": float,
        "weight_decay": float,
        "num_epochs": int,
        "warmup_steps": int,
        # Generation configuration
        "temperature": float,
        "top_k": int,
        "top_p": float,
        "max_new_tokens": int,
    }

    converted_config = {}
    for key, value in config.items():
        if key in type_conversions:
            try:
                converted_config[key] = type_conversions[key](value)
            except (ValueError, TypeError):
                print(
                    f"‚ö†Ô∏è  Warning: Could not convert {key}={value} to {type_conversions[key].__name__}, keeping as string"
                )
                converted_config[key] = value
        else:
            # Keep other values as they are (strings, etc.)
            converted_config[key] = value

    return converted_config


def get_default_config():
    """Return default configuration if YAML file is not available"""
    return {
        # Data configuration
        "data_folder": "data/clean_data",
        "tokenizer_file": "vietnamese_tokenizer.json",
        "vocab_size": 25000,
        "max_seq_len": 512,
        "train_split": 0.8,
        # Model configuration
        "d_model": 768,
        "n_heads": 12,
        "n_layers": 12,
        "d_ff": 3072,
        "dropout": 0.1,
        # Training configuration
        "batch_size": 16,
        "learning_rate": 3e-5,
        "weight_decay": 0.01,
        "num_epochs": 50,
        "warmup_steps": 5000,
        "device": "auto",  # 'cuda', 'cpu', 'mps', or 'auto'
        # Generation configuration
        "temperature": 0.8,
        "top_k": 10,
        "top_p": 0.9,
        "max_new_tokens": 256,
        # Save configuration
        "model_save_path": "vietnamese_transformer_best.pt",
        "config_save_path": "training_config.json",
    }


def create_sample_data(file_path: str):
    """Create sample Vietnamese literature data if file doesn't exist"""
    sample_data = (
        """
    TrƒÉm nƒÉm trong c√µi ng∆∞·ªùi ta,
    Ch·ªØ t√†i ch·ªØ m·ªánh kh√©o l√† gh√©t nhau.
    Tr·∫£i qua m·ªôt cu·ªôc b·ªÉ d√¢u,
    Nh·ªØng ƒëi·ªÅu tr√¥ng th·∫•y m√† ƒëau ƒë·ªõn l√≤ng.
    L·∫° g√¨ b·ªâ s·∫Øc t∆∞ phong,
    Tr·ªùi xanh quen th√≥i m√° h·ªìng ƒë√°nh ghen.
    C≈©ng ƒë√†nh r·∫±ng s·ªë ki·∫øp en,
    V√¨ ch∆∞ng n√†ng s·∫Øc n√™n th√™m n√†ng t√†i.

    Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt b·ªüi Nguy·ªÖn Du v√†o th·∫ø k·ª∑ XIX.
    T√°c ph·∫©m n√†y ƒë∆∞·ª£c coi l√† ki·ªát t√°c c·ªßa vƒÉn h·ªçc Vi·ªát Nam.
    C√¢u chuy·ªán k·ªÉ v·ªÅ s·ªë ph·∫≠n c·ªßa Th√∫y Ki·ªÅu, m·ªôt c√¥ g√°i t√†i s·∫Øc v∆∞·ª£t tr·ªôi.
    Qua nhi·ªÅu thƒÉng tr·∫ßm trong cu·ªôc ƒë·ªùi, cu·ªëi c√πng n√†ng ƒë∆∞·ª£c ƒëo√†n t·ª• v·ªõi gia ƒë√¨nh.

    VƒÉn h·ªçc Vi·ªát Nam c√≥ nhi·ªÅu t√°c ph·∫©m n·ªïi ti·∫øng kh√°c.
    S·ªë ƒê·ªè c·ªßa V≈© Tr·ªçng Ph·ª•ng l√† m·ªôt ti·ªÉu thuy·∫øt hi·ªán th·ª±c xu·∫•t s·∫Øc.
    Kim L√¢n n·ªïi ti·∫øng v·ªõi nh·ªØng truy·ªán ng·∫Øn v·ªÅ cu·ªôc s·ªëng n√¥ng th√¥n.
    Nguy·ªÖn Tu√¢n ƒë∆∞·ª£c bi·∫øt ƒë·∫øn v·ªõi vƒÉn xu√¥i mi√™u t·∫£ thi√™n nhi√™n tuy·ªát ƒë·∫πp.

    Th∆° ca c·ªï ƒëi·ªÉn Vi·ªát Nam th∆∞·ªùng s·ª≠ d·ª•ng th·ªÉ l·ª•c b√°t, th·∫•t ng√¥n t·ª© tuy·ªát.
    Truy·ªÅn th·ªëng vƒÉn h·ªçc d√¢n gian r·∫•t phong ph√∫ v·ªõi c√°c c√¢u chuy·ªán c·ªï t√≠ch.
    T·∫•m C√°m, Th·∫°ch Sanh, S∆°n Tinh Th·ªßy Tinh l√† nh·ªØng truy·ªán n·ªïi ti·∫øng.
    C√°c c√¢u ca dao, t·ª•c ng·ªØ c≈©ng th·ªÉ hi·ªán tri·∫øt l√Ω s·ªëng s√¢u s·∫Øc.

    H·ªì Ch√≠ Minh c≈©ng c√≥ nh·ªØng b√†i th∆° n·ªïi ti·∫øng vi·∫øt trong t√π.
    Nh·∫≠t k√Ω trong t√π th·ªÉ hi·ªán tinh th·∫ßn ki√™n c∆∞·ªùng c·ªßa ng∆∞·ªùi c·ªông s·∫£n.
    VƒÉn h·ªçc hi·ªán ƒë·∫°i Vi·ªát Nam ph√°t tri·ªÉn m·∫°nh m·∫Ω t·ª´ ƒë·∫ßu th·∫ø k·ª∑ XX.
    Nhi·ªÅu t√°c gi·∫£ tr·∫ª ƒë√£ g√≥p ph·∫ßn l√†m gi√†u kho t√†ng vƒÉn h·ªçc d√¢n t·ªôc.

    Ng√¥n ng·ªØ Vi·ªát Nam c√≥ ƒë·∫∑c ƒëi·ªÉm l√† ƒë∆°n √¢m ti·∫øt v√† c√≥ thanh ƒëi·ªáu.
    M·ªói ti·∫øng c√≥ th·ªÉ mang nhi·ªÅu nghƒ©a kh√°c nhau t√πy theo thanh ƒëi·ªáu.
    ƒêi·ªÅu n√†y t·∫°o n√™n s·ª± phong ph√∫ v√† ƒëa d·∫°ng trong c√°ch di·ªÖn ƒë·∫°t.
    VƒÉn h·ªçc Vi·ªát Nam khai th√°c tri·ªát ƒë·ªÉ v·∫ª ƒë·∫πp c·ªßa ng√¥n ng·ªØ n√†y.

    Truy·ªán Ki·ªÅu kh√¥ng ch·ªâ l√† t√°c ph·∫©m vƒÉn h·ªçc m√† c√≤n l√† b·ª©c tranh x√£ h·ªôi.
    N√≥ ph·∫£n √°nh nh·ªØng m√¢u thu·∫´n s√¢u s·∫Øc c·ªßa x√£ h·ªôi phong ki·∫øn.
    S·ªë ph·∫≠n con ng∆∞·ªùi b·ªã chi ph·ªëi b·ªüi ho√†n c·∫£nh x√£ h·ªôi.
    T√¨nh y√™u v√† l√≤ng hi·∫øu th·∫£o l√† nh·ªØng gi√° tr·ªã ƒë∆∞·ª£c t√¥n vinh.

    Ng√†y nay, vƒÉn h·ªçc Vi·ªát Nam ti·∫øp t·ª•c ph√°t tri·ªÉn v·ªõi nhi·ªÅu h√¨nh th·ª©c m·ªõi.
    Ti·ªÉu thuy·∫øt, truy·ªán ng·∫Øn, th∆°, k·ªãch ƒë·ªÅu c√≥ nh·ªØng t√°c ph·∫©m xu·∫•t s·∫Øc.
    C√°c nh√† vƒÉn tr·∫ª mang ƒë·∫øn l√†n gi√≥ m·ªõi cho n·ªÅn vƒÉn h·ªçc.
    VƒÉn h·ªçc Vi·ªát Nam ng√†y c√†ng ƒë∆∞·ª£c qu·ªëc t·∫ø quan t√¢m v√† ƒë√°nh gi√° cao.
    """
        * 3
    )  # Repeat to have more training data

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(sample_data)
    print(f"‚úÖ Created sample data file: {file_path}")


def plot_training_history(train_losses, val_losses, save_path="training_history.png"):
    """Plot and save training history"""
    plt.figure(figsize=(10, 6))

    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, "b-", label="Training Loss", linewidth=2)
    plt.plot(epochs, val_losses, "r-", label="Validation Loss", linewidth=2)

    plt.title("Vietnamese Transformer Training History", fontsize=16, fontweight="bold")
    plt.xlabel("Epoch", fontsize=12)
    plt.ylabel("Loss", fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # Add annotations
    min_val_loss_epoch = val_losses.index(min(val_losses)) + 1
    plt.annotate(
        f"Best Val Loss: {min(val_losses):.4f}\nEpoch: {min_val_loss_epoch}",
        xy=(min_val_loss_epoch, min(val_losses)),
        xytext=(min_val_loss_epoch + len(val_losses) * 0.1, min(val_losses) + 0.1),
        arrowprops=dict(arrowstyle="->", color="red"),
        fontsize=10,
        ha="left",
    )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"üìä Training history saved to: {save_path}")


def test_generation(
    model: VietnameseTransformer, tokenizer, device, test_cases=None, max_new_tokens=15
):
    """Test text generation with various examples"""
    if test_cases is None:
        test_cases = [
            "Truy·ªán Ki·ªÅu ƒë∆∞·ª£c vi·∫øt",
            "VƒÉn h·ªçc Vi·ªát Nam",
            "Nguy·ªÖn Du l√†",
            "Th√∫y Ki·ªÅu",
            "T√°c ph·∫©m n√†y",
        ]

    print("\n" + "=" * 60)
    print("üéØ TESTING TEXT GENERATION")
    print("=" * 60)

    model.eval()

    for i, prompt in enumerate(test_cases, 1):
        print(f"\n--- Test {i} ---")
        print(f"Input: '{prompt}'")

        prompt = preprocessor.word_segment(prompt)

        # Encode input
        input_ids = torch.tensor(
            [tokenizer.encode(prompt, add_special_tokens=False).ids], device=device
        )

        # Generate with different settings
        generation_configs = [
            {
                "temperature": 0.7,
                "top_k": 50,
                "top_p": 0.9,
                "max_new_tokens": max_new_tokens,
                "name": "Balanced",
            },
            {
                "temperature": 1.0,
                "top_k": 20,
                "top_p": 0.8,
                "max_new_tokens": max_new_tokens,
                "name": "Creative",
            },
            {
                "temperature": 0.3,
                "top_k": 10,
                "top_p": 1.0,
                "max_new_tokens": max_new_tokens,
                "name": "Conservative",
            },
        ]

        for config in generation_configs:
            with torch.no_grad():
                generated = model.generate(
                    input_ids,
                    temperature=config["temperature"],
                    top_k=config["top_k"],
                    top_p=config["top_p"],
                    max_new_tokens=config["max_new_tokens"],
                    do_sample=True,
                )

            generated_text = (
                tokenizer.decode(generated[0].cpu().tolist())
                .replace(" ##", "")
                .replace("_", " ")
                .replace("[LF]", "\n")
            )
            print(f"  {config['name']}: '{generated_text}'")


def generate_text(
    prompt: str,
    model: VietnameseTransformer,
    tokenizer,
    device,
    temperature=0.5,
    top_k=20,
    top_p=0.9,
    max_new_tokens=15,
):
    model.eval()
    prompt = preprocessor.clean_text(prompt)
    prompt = preprocessor.word_segment(prompt)
    input_ids = torch.tensor(
        [tokenizer.encode(prompt, add_special_tokens=False).ids]
    ).to(device)

    config = {
        "temperature": temperature,
        "top_k": top_k,
        "top_p": top_p,
        "max_new_tokens": max_new_tokens,
        "name": "Balanced",
    }

    with torch.no_grad():
        generated = model.generate(
            input_ids,
            temperature=config["temperature"],
            top_k=config["top_k"],
            top_p=config["top_p"],
            max_new_tokens=config["max_new_tokens"],
            do_sample=True,
        )

        generated_text = tokenizer.decode(generated[0].cpu().tolist())
        return generated_text.replace(" ##", "").replace("_", " ").replace("[LF]", "\n")


def save_model_and_config(model, tokenizer, config, model_path, config_path):
    """Save the trained model and configuration"""
    # Save model state
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "model_config": {
                "vocab_size": config["vocab_size"],
                "d_model": config["d_model"],
                "n_heads": config["n_heads"],
                "n_layers": config["n_layers"],
                "d_ff": config["d_ff"],
                "max_seq_len": config["max_seq_len"],
                "dropout": config["dropout"],
            },
        },
        model_path,
    )

    # Save configuration
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"üíæ Model saved to: {model_path}")
    print(f"‚öôÔ∏è  Configuration saved to: {config_path}")


def load_model_and_tokenizer(model_path, tokenizer_path):
    """Load trained model and tokenizer"""
    # Load model checkpoint
    checkpoint = torch.load(model_path, map_location="cpu")
    model_config = checkpoint["model_config"]

    # Create model
    model = VietnameseTransformer(**model_config)
    model.load_state_dict(checkpoint["model_state_dict"])

    # Load tokenizer
    tokenizer = VietnameseTokenizer()
    tokenizer.load(tokenizer_path)

    print(f"‚úÖ Model loaded from: {model_path}")
    print(f"‚úÖ Tokenizer loaded from: {tokenizer_path}")

    return model, tokenizer
